
==> Audit <==
|---------|----------------|----------|--------------|---------|---------------------|---------------------|
| Command |      Args      | Profile  |     User     | Version |     Start Time      |      End Time       |
|---------|----------------|----------|--------------|---------|---------------------|---------------------|
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 12:37 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 12:45 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 13:41 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 13:43 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 13:55 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 13:55 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 13:56 GMT |                     |
| start   |                | minikube | joshuasackey | v1.36.0 | 30 Jul 25 22:15 GMT | 30 Jul 25 22:20 GMT |
| start   |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 10:53 GMT | 31 Jul 25 10:54 GMT |
| stop    |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 11:24 GMT | 31 Jul 25 11:24 GMT |
| start   |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 11:25 GMT | 31 Jul 25 11:26 GMT |
| stop    |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 11:35 GMT | 31 Jul 25 11:35 GMT |
| start   |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 11:36 GMT | 31 Jul 25 11:37 GMT |
| stop    |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 12:15 GMT | 31 Jul 25 12:15 GMT |
| start   |                | minikube | joshuasackey | v1.36.0 | 31 Jul 25 12:26 GMT | 31 Jul 25 12:28 GMT |
| addons  | list           | minikube | joshuasackey | v1.36.0 | 01 Aug 25 17:12 GMT | 01 Aug 25 17:12 GMT |
| addons  | enable ingress | minikube | joshuasackey | v1.36.0 | 01 Aug 25 17:24 GMT |                     |
| addons  | enable ingress | minikube | joshuasackey | v1.36.0 | 01 Aug 25 17:27 GMT |                     |
|---------|----------------|----------|--------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/31 12:26:54
Running on machine: Joshuas-MacBook-Pro
Binary: Built with gc go1.24.0 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0731 12:26:54.051031   37158 out.go:345] Setting OutFile to fd 1 ...
I0731 12:26:54.052168   37158 out.go:397] isatty.IsTerminal(1) = true
I0731 12:26:54.052184   37158 out.go:358] Setting ErrFile to fd 2...
I0731 12:26:54.052190   37158 out.go:397] isatty.IsTerminal(2) = true
I0731 12:26:54.052883   37158 root.go:338] Updating PATH: /Users/joshuasackey/.minikube/bin
W0731 12:26:54.054074   37158 root.go:314] Error reading config file at /Users/joshuasackey/.minikube/config/config.json: open /Users/joshuasackey/.minikube/config/config.json: no such file or directory
I0731 12:26:54.058154   37158 out.go:352] Setting JSON to false
I0731 12:26:54.106521   37158 start.go:130] hostinfo: {"hostname":"Joshuas-MacBook-Pro.local","uptime":1818835,"bootTime":1752145979,"procs":800,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.5","kernelVersion":"24.5.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e318c8e2-a15c-5446-8b14-0dbcefc46fa8"}
W0731 12:26:54.107952   37158 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0731 12:26:54.129265   37158 out.go:177] üòÑ  minikube v1.36.0 on Darwin 15.5
I0731 12:26:54.174404   37158 notify.go:220] Checking for updates...
I0731 12:26:54.175976   37158 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0731 12:26:54.178630   37158 driver.go:404] Setting default libvirt URI to qemu:///system
I0731 12:26:54.234023   37158 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)
I0731 12:26:54.234655   37158 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0731 12:26:54.758682   37158 info.go:266] docker info: {ID:6f0461ea-8b1e-4231-964c-d650b9b7b504 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:12 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:62 OomKillDisable:false NGoroutines:91 SystemTime:2025-07-31 12:26:54.743450431 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8220102656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/joshuasackey/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/joshuasackey/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:/Users/joshuasackey/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:/Users/joshuasackey/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:/Users/joshuasackey/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:/Users/joshuasackey/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/joshuasackey/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:/Users/joshuasackey/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/joshuasackey/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/Users/joshuasackey/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/joshuasackey/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:sbom Path:/Users/joshuasackey/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/joshuasackey/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0731 12:26:54.779053   37158 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0731 12:26:54.817767   37158 start.go:304] selected driver: docker
I0731 12:26:54.817803   37158 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:kicbase/stable:v0.0.47 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0731 12:26:54.818049   37158 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0731 12:26:54.818676   37158 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0731 12:26:54.995154   37158 info.go:266] docker info: {ID:6f0461ea-8b1e-4231-964c-d650b9b7b504 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:12 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:62 OomKillDisable:false NGoroutines:91 SystemTime:2025-07-31 12:26:54.980681494 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8220102656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/joshuasackey/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/joshuasackey/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:/Users/joshuasackey/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:/Users/joshuasackey/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:/Users/joshuasackey/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:/Users/joshuasackey/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/joshuasackey/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:/Users/joshuasackey/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/joshuasackey/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/Users/joshuasackey/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/joshuasackey/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:sbom Path:/Users/joshuasackey/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/joshuasackey/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0731 12:26:54.995623   37158 cni.go:84] Creating CNI manager for ""
I0731 12:26:54.996141   37158 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0731 12:26:54.996203   37158 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:kicbase/stable:v0.0.47 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0731 12:26:55.015323   37158 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0731 12:26:55.052804   37158 cache.go:121] Beginning downloading kic base image for docker with docker
I0731 12:26:55.072154   37158 out.go:177] üöú  Pulling base image v0.0.47 ...
I0731 12:26:55.110199   37158 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0731 12:26:55.110361   37158 preload.go:146] Found local preload: /Users/joshuasackey/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0731 12:26:55.110383   37158 cache.go:56] Caching tarball of preloaded images
I0731 12:26:55.110834   37158 preload.go:172] Found /Users/joshuasackey/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0731 12:26:55.110855   37158 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0731 12:26:55.111013   37158 image.go:81] Checking for kicbase/stable:v0.0.47 in local docker daemon
I0731 12:26:55.111229   37158 profile.go:143] Saving config to /Users/joshuasackey/.minikube/profiles/minikube/config.json ...
W0731 12:26:55.366820   37158 image.go:93] failed to get config for kicbase/stable:v0.0.47: parsing time "" as "2006-01-02T15:04:05.999999999Z07:00": cannot parse "" as "2006"
I0731 12:26:55.366853   37158 cache.go:150] Downloading kicbase/stable:v0.0.47 to local cache
I0731 12:26:55.367062   37158 image.go:65] Checking for kicbase/stable:v0.0.47 in local cache directory
I0731 12:26:55.367081   37158 image.go:68] Found kicbase/stable:v0.0.47 in local cache directory, skipping pull
I0731 12:26:55.367088   37158 image.go:137] kicbase/stable:v0.0.47 exists in cache, skipping pull
I0731 12:26:55.367101   37158 cache.go:153] successfully saved kicbase/stable:v0.0.47 as a tarball
I0731 12:26:55.367105   37158 cache.go:163] Loading kicbase/stable:v0.0.47 from local cache
I0731 12:27:58.622004   37158 cache.go:165] successfully loaded and using kicbase/stable:v0.0.47 from cached tarball
I0731 12:27:58.622587   37158 cache.go:230] Successfully downloaded all kic artifacts
I0731 12:27:58.625130   37158 start.go:360] acquireMachinesLock for minikube: {Name:mk1e481c4143e932a31c0d181e017602d43768c8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0731 12:27:58.625384   37158 start.go:364] duration metric: took 189.072¬µs to acquireMachinesLock for "minikube"
I0731 12:27:58.625869   37158 start.go:96] Skipping create...Using existing machine configuration
I0731 12:27:58.626282   37158 fix.go:54] fixHost starting: 
I0731 12:27:58.629102   37158 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0731 12:27:58.667048   37158 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0731 12:27:58.667114   37158 fix.go:138] unexpected machine state, will restart: <nil>
I0731 12:27:58.696317   37158 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0731 12:27:58.734677   37158 cli_runner.go:164] Run: docker start minikube
I0731 12:27:59.029477   37158 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0731 12:27:59.082000   37158 kic.go:430] container "minikube" state is running.
I0731 12:27:59.084800   37158 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0731 12:27:59.121586   37158 profile.go:143] Saving config to /Users/joshuasackey/.minikube/profiles/minikube/config.json ...
I0731 12:27:59.122392   37158 machine.go:93] provisionDockerMachine start ...
I0731 12:27:59.123438   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:27:59.161996   37158 main.go:141] libmachine: Using SSH client type: native
I0731 12:27:59.165453   37158 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106e09c80] 0x106e0c980 <nil>  [] 0s} 127.0.0.1 53318 <nil> <nil>}
I0731 12:27:59.165467   37158 main.go:141] libmachine: About to run SSH command:
hostname
I0731 12:27:59.168192   37158 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0731 12:28:02.349159   37158 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0731 12:28:02.350219   37158 ubuntu.go:169] provisioning hostname "minikube"
I0731 12:28:02.350836   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:02.391039   37158 main.go:141] libmachine: Using SSH client type: native
I0731 12:28:02.391327   37158 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106e09c80] 0x106e0c980 <nil>  [] 0s} 127.0.0.1 53318 <nil> <nil>}
I0731 12:28:02.391336   37158 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0731 12:28:02.567633   37158 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0731 12:28:02.568714   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:02.604378   37158 main.go:141] libmachine: Using SSH client type: native
I0731 12:28:02.604647   37158 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106e09c80] 0x106e0c980 <nil>  [] 0s} 127.0.0.1 53318 <nil> <nil>}
I0731 12:28:02.604656   37158 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0731 12:28:02.765593   37158 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0731 12:28:02.765645   37158 ubuntu.go:175] set auth options {CertDir:/Users/joshuasackey/.minikube CaCertPath:/Users/joshuasackey/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/joshuasackey/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/joshuasackey/.minikube/machines/server.pem ServerKeyPath:/Users/joshuasackey/.minikube/machines/server-key.pem ClientKeyPath:/Users/joshuasackey/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/joshuasackey/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/joshuasackey/.minikube}
I0731 12:28:02.765682   37158 ubuntu.go:177] setting up certificates
I0731 12:28:02.766653   37158 provision.go:84] configureAuth start
I0731 12:28:02.766934   37158 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0731 12:28:02.806328   37158 provision.go:143] copyHostCerts
I0731 12:28:02.808209   37158 exec_runner.go:144] found /Users/joshuasackey/.minikube/ca.pem, removing ...
I0731 12:28:02.808520   37158 exec_runner.go:203] rm: /Users/joshuasackey/.minikube/ca.pem
I0731 12:28:02.808680   37158 exec_runner.go:151] cp: /Users/joshuasackey/.minikube/certs/ca.pem --> /Users/joshuasackey/.minikube/ca.pem (1094 bytes)
I0731 12:28:02.809866   37158 exec_runner.go:144] found /Users/joshuasackey/.minikube/cert.pem, removing ...
I0731 12:28:02.809873   37158 exec_runner.go:203] rm: /Users/joshuasackey/.minikube/cert.pem
I0731 12:28:02.809997   37158 exec_runner.go:151] cp: /Users/joshuasackey/.minikube/certs/cert.pem --> /Users/joshuasackey/.minikube/cert.pem (1139 bytes)
I0731 12:28:02.810631   37158 exec_runner.go:144] found /Users/joshuasackey/.minikube/key.pem, removing ...
I0731 12:28:02.810634   37158 exec_runner.go:203] rm: /Users/joshuasackey/.minikube/key.pem
I0731 12:28:02.810721   37158 exec_runner.go:151] cp: /Users/joshuasackey/.minikube/certs/key.pem --> /Users/joshuasackey/.minikube/key.pem (1679 bytes)
I0731 12:28:02.811287   37158 provision.go:117] generating server cert: /Users/joshuasackey/.minikube/machines/server.pem ca-key=/Users/joshuasackey/.minikube/certs/ca.pem private-key=/Users/joshuasackey/.minikube/certs/ca-key.pem org=joshuasackey.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0731 12:28:02.916930   37158 provision.go:177] copyRemoteCerts
I0731 12:28:02.917541   37158 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0731 12:28:02.917637   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:02.950781   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:03.069177   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0731 12:28:03.105594   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0731 12:28:03.140001   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0731 12:28:03.171726   37158 provision.go:87] duration metric: took 405.038634ms to configureAuth
I0731 12:28:03.171745   37158 ubuntu.go:193] setting minikube options for container-runtime
I0731 12:28:03.171957   37158 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0731 12:28:03.172084   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:03.205362   37158 main.go:141] libmachine: Using SSH client type: native
I0731 12:28:03.205624   37158 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106e09c80] 0x106e0c980 <nil>  [] 0s} 127.0.0.1 53318 <nil> <nil>}
I0731 12:28:03.205629   37158 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0731 12:28:03.364586   37158 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0731 12:28:03.365632   37158 ubuntu.go:71] root file system type: overlay
I0731 12:28:03.366888   37158 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0731 12:28:03.367115   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:03.405367   37158 main.go:141] libmachine: Using SSH client type: native
I0731 12:28:03.405639   37158 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106e09c80] 0x106e0c980 <nil>  [] 0s} 127.0.0.1 53318 <nil> <nil>}
I0731 12:28:03.405692   37158 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0731 12:28:03.585591   37158 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0731 12:28:03.586723   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:03.620948   37158 main.go:141] libmachine: Using SSH client type: native
I0731 12:28:03.621198   37158 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106e09c80] 0x106e0c980 <nil>  [] 0s} 127.0.0.1 53318 <nil> <nil>}
I0731 12:28:03.621209   37158 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0731 12:28:03.780234   37158 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0731 12:28:03.780269   37158 machine.go:96] duration metric: took 4.657930241s to provisionDockerMachine
I0731 12:28:03.781176   37158 start.go:293] postStartSetup for "minikube" (driver="docker")
I0731 12:28:03.781196   37158 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0731 12:28:03.781397   37158 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0731 12:28:03.781483   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:03.813933   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:03.931182   37158 ssh_runner.go:195] Run: cat /etc/os-release
I0731 12:28:03.939191   37158 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0731 12:28:03.939244   37158 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0731 12:28:03.939257   37158 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0731 12:28:03.939264   37158 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0731 12:28:03.939278   37158 filesync.go:126] Scanning /Users/joshuasackey/.minikube/addons for local assets ...
I0731 12:28:03.939460   37158 filesync.go:126] Scanning /Users/joshuasackey/.minikube/files for local assets ...
I0731 12:28:03.939542   37158 start.go:296] duration metric: took 158.349822ms for postStartSetup
I0731 12:28:03.939684   37158 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0731 12:28:03.939768   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:03.973041   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:04.075889   37158 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0731 12:28:04.086048   37158 fix.go:56] duration metric: took 5.459839641s for fixHost
I0731 12:28:04.086066   37158 start.go:83] releasing machines lock for "minikube", held for 5.460742774s
I0731 12:28:04.086228   37158 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0731 12:28:04.119188   37158 ssh_runner.go:195] Run: cat /version.json
I0731 12:28:04.119296   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:04.121424   37158 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0731 12:28:04.122726   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:04.152699   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:04.154814   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:04.260813   37158 ssh_runner.go:195] Run: systemctl --version
I0731 12:28:05.144429   37158 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.022978217s)
I0731 12:28:05.145169   37158 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0731 12:28:05.158551   37158 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0731 12:28:05.188060   37158 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0731 12:28:05.188212   37158 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0731 12:28:05.203277   37158 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0731 12:28:05.203292   37158 start.go:495] detecting cgroup driver to use...
I0731 12:28:05.203305   37158 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0731 12:28:05.205693   37158 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0731 12:28:05.230348   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0731 12:28:05.246212   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0731 12:28:05.263939   37158 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0731 12:28:05.264093   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0731 12:28:05.278851   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0731 12:28:05.294259   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0731 12:28:05.310792   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0731 12:28:05.331942   37158 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0731 12:28:05.346721   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0731 12:28:05.361987   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0731 12:28:05.383306   37158 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0731 12:28:05.399438   37158 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0731 12:28:05.412934   37158 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0731 12:28:05.427445   37158 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0731 12:28:05.499534   37158 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0731 12:28:05.608881   37158 start.go:495] detecting cgroup driver to use...
I0731 12:28:05.608918   37158 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0731 12:28:05.609581   37158 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0731 12:28:05.646739   37158 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0731 12:28:05.646996   37158 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0731 12:28:05.665492   37158 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0731 12:28:05.691791   37158 ssh_runner.go:195] Run: which cri-dockerd
I0731 12:28:05.701359   37158 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0731 12:28:05.715403   37158 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0731 12:28:05.741968   37158 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0731 12:28:06.056904   37158 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0731 12:28:06.168681   37158 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0731 12:28:06.169089   37158 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0731 12:28:06.194610   37158 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0731 12:28:06.209603   37158 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0731 12:28:06.286857   37158 ssh_runner.go:195] Run: sudo systemctl restart docker
I0731 12:28:08.690688   37158 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.403842348s)
I0731 12:28:08.690865   37158 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0731 12:28:08.705707   37158 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0731 12:28:08.721592   37158 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0731 12:28:08.736788   37158 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0731 12:28:08.811656   37158 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0731 12:28:08.881388   37158 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0731 12:28:08.960141   37158 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0731 12:28:08.987119   37158 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0731 12:28:09.002143   37158 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0731 12:28:09.074632   37158 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0731 12:28:09.218999   37158 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0731 12:28:09.235654   37158 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0731 12:28:09.237689   37158 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0731 12:28:09.243596   37158 start.go:563] Will wait 60s for crictl version
I0731 12:28:09.243727   37158 ssh_runner.go:195] Run: which crictl
I0731 12:28:09.249733   37158 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0731 12:28:09.292909   37158 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0731 12:28:09.293020   37158 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0731 12:28:09.321622   37158 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0731 12:28:09.370432   37158 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0731 12:28:09.371434   37158 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0731 12:28:09.509880   37158 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0731 12:28:09.511470   37158 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0731 12:28:09.517969   37158 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0731 12:28:09.533995   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0731 12:28:09.569554   37158 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:kicbase/stable:v0.0.47 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0731 12:28:09.570520   37158 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0731 12:28:09.570644   37158 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0731 12:28:09.597558   37158 docker.go:702] Got preloaded images: -- stdout --
joey727/auth-service:new
joey727/auth-service:latest
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0731 12:28:09.597601   37158 docker.go:632] Images already preloaded, skipping extraction
I0731 12:28:09.598278   37158 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0731 12:28:09.622647   37158 docker.go:702] Got preloaded images: -- stdout --
joey727/auth-service:new
joey727/auth-service:latest
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0731 12:28:09.622673   37158 cache_images.go:84] Images are preloaded, skipping loading
I0731 12:28:09.622688   37158 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0731 12:28:09.624097   37158 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0731 12:28:09.624213   37158 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0731 12:28:09.686065   37158 cni.go:84] Creating CNI manager for ""
I0731 12:28:09.686083   37158 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0731 12:28:09.686099   37158 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0731 12:28:09.686138   37158 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0731 12:28:09.686290   37158 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0731 12:28:09.686436   37158 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0731 12:28:09.700449   37158 binaries.go:44] Found k8s binaries, skipping transfer
I0731 12:28:09.700602   37158 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0731 12:28:09.713563   37158 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0731 12:28:09.736961   37158 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0731 12:28:09.760437   37158 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0731 12:28:09.783981   37158 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0731 12:28:09.790218   37158 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0731 12:28:09.809060   37158 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0731 12:28:09.878605   37158 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0731 12:28:09.921619   37158 certs.go:68] Setting up /Users/joshuasackey/.minikube/profiles/minikube for IP: 192.168.49.2
I0731 12:28:09.921665   37158 certs.go:194] generating shared ca certs ...
I0731 12:28:09.921681   37158 certs.go:226] acquiring lock for ca certs: {Name:mk161e5d522f9639b3e122b22b53c6afedf8bd0f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0731 12:28:09.922679   37158 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/joshuasackey/.minikube/ca.key
I0731 12:28:09.923240   37158 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/joshuasackey/.minikube/proxy-client-ca.key
I0731 12:28:09.923256   37158 certs.go:256] generating profile certs ...
I0731 12:28:09.923803   37158 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/joshuasackey/.minikube/profiles/minikube/client.key
I0731 12:28:09.924342   37158 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/joshuasackey/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0731 12:28:09.924781   37158 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/joshuasackey/.minikube/profiles/minikube/proxy-client.key
I0731 12:28:09.925538   37158 certs.go:484] found cert: /Users/joshuasackey/.minikube/certs/ca-key.pem (1675 bytes)
I0731 12:28:09.925610   37158 certs.go:484] found cert: /Users/joshuasackey/.minikube/certs/ca.pem (1094 bytes)
I0731 12:28:09.925650   37158 certs.go:484] found cert: /Users/joshuasackey/.minikube/certs/cert.pem (1139 bytes)
I0731 12:28:09.925677   37158 certs.go:484] found cert: /Users/joshuasackey/.minikube/certs/key.pem (1679 bytes)
I0731 12:28:09.930939   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0731 12:28:09.965080   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0731 12:28:10.024570   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0731 12:28:10.068253   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0731 12:28:10.251273   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0731 12:28:10.354365   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0731 12:28:10.456764   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0731 12:28:10.558747   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0731 12:28:10.642886   37158 ssh_runner.go:362] scp /Users/joshuasackey/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0731 12:28:10.730318   37158 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0731 12:28:10.839901   37158 ssh_runner.go:195] Run: openssl version
I0731 12:28:10.854529   37158 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0731 12:28:10.875724   37158 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0731 12:28:10.929774   37158 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 30 22:20 /usr/share/ca-certificates/minikubeCA.pem
I0731 12:28:10.929938   37158 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0731 12:28:10.945136   37158 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0731 12:28:10.962566   37158 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0731 12:28:10.974034   37158 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0731 12:28:11.047388   37158 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0731 12:28:11.062354   37158 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0731 12:28:11.132035   37158 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0731 12:28:11.158020   37158 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0731 12:28:11.243009   37158 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0731 12:28:11.326225   37158 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:kicbase/stable:v0.0.47 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0731 12:28:11.326658   37158 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0731 12:28:11.429022   37158 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0731 12:28:11.461684   37158 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0731 12:28:11.461711   37158 kubeadm.go:589] restartPrimaryControlPlane start ...
I0731 12:28:11.461897   37158 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0731 12:28:11.544806   37158 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0731 12:28:11.544976   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0731 12:28:11.587525   37158 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /Users/joshuasackey/.kube/config
I0731 12:28:11.587701   37158 kubeconfig.go:62] /Users/joshuasackey/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0731 12:28:11.590669   37158 lock.go:35] WriteFile acquiring /Users/joshuasackey/.kube/config: {Name:mkef81b380d13689e70e9693524b5777d23a31ac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0731 12:28:11.615834   37158 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0731 12:28:11.643330   37158 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0731 12:28:11.643390   37158 kubeadm.go:593] duration metric: took 181.658093ms to restartPrimaryControlPlane
I0731 12:28:11.643400   37158 kubeadm.go:394] duration metric: took 317.203108ms to StartCluster
I0731 12:28:11.643424   37158 settings.go:142] acquiring lock: {Name:mkad4897c99d277bcdbbad10bdb89771969d6a90 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0731 12:28:11.643588   37158 settings.go:150] Updating kubeconfig:  /Users/joshuasackey/.kube/config
I0731 12:28:11.644702   37158 lock.go:35] WriteFile acquiring /Users/joshuasackey/.kube/config: {Name:mkef81b380d13689e70e9693524b5777d23a31ac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0731 12:28:11.645720   37158 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0731 12:28:11.645862   37158 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0731 12:28:11.647145   37158 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0731 12:28:11.648149   37158 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0731 12:28:11.648181   37158 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0731 12:28:11.648234   37158 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0731 12:28:11.648643   37158 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0731 12:28:11.648656   37158 addons.go:247] addon storage-provisioner should already be in state true
I0731 12:28:11.648916   37158 host.go:66] Checking if "minikube" exists ...
I0731 12:28:11.649622   37158 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0731 12:28:11.649838   37158 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0731 12:28:11.669698   37158 out.go:177] üîé  Verifying Kubernetes components...
I0731 12:28:11.726569   37158 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0731 12:28:11.742044   37158 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0731 12:28:11.742074   37158 addons.go:247] addon default-storageclass should already be in state true
I0731 12:28:11.742109   37158 host.go:66] Checking if "minikube" exists ...
I0731 12:28:11.743220   37158 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0731 12:28:11.761821   37158 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0731 12:28:11.780510   37158 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0731 12:28:11.780518   37158 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0731 12:28:11.780657   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:11.791723   37158 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0731 12:28:11.791742   37158 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0731 12:28:11.791893   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0731 12:28:11.817816   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:11.830248   37158 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53318 SSHKeyPath:/Users/joshuasackey/.minikube/machines/minikube/id_rsa Username:docker}
I0731 12:28:12.243902   37158 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0731 12:28:12.247412   37158 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0731 12:28:12.348938   37158 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0731 12:28:12.950262   37158 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0731 12:28:12.950530   37158 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0731 12:28:12.952155   37158 retry.go:31] will retry after 285.22801ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0731 12:28:12.987519   37158 api_server.go:52] waiting for apiserver process to appear ...
I0731 12:28:12.988229   37158 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0731 12:28:13.032134   37158 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0731 12:28:13.032350   37158 retry.go:31] will retry after 133.604702ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0731 12:28:13.132770   37158 api_server.go:72] duration metric: took 1.486878059s to wait for apiserver process to appear ...
I0731 12:28:13.132814   37158 api_server.go:88] waiting for apiserver healthz status ...
I0731 12:28:13.132883   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:13.141470   37158 api_server.go:269] stopped: https://127.0.0.1:53322/healthz: Get "https://127.0.0.1:53322/healthz": EOF
I0731 12:28:13.166430   37158 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0731 12:28:13.238388   37158 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0731 12:28:13.633847   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:16.939441   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0731 12:28:16.939496   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0731 12:28:16.939526   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:17.039168   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0731 12:28:17.039187   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0731 12:28:17.133194   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:17.229499   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0731 12:28:17.229523   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0731 12:28:17.635767   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:17.740041   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:17.740071   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:17.843218   37158 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (4.676811593s)
I0731 12:28:18.133434   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:18.247605   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:18.247632   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:18.633537   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:18.743153   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:18.743178   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:19.133395   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:19.227500   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:19.227555   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:19.634385   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:19.655311   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:19.655333   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:20.133326   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:20.149077   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:20.149097   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:20.632936   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:20.651171   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:20.651190   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:21.133035   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:21.150347   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:21.150379   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:21.634118   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:21.734998   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0731 12:28:21.735027   37158 api_server.go:103] status: https://127.0.0.1:53322/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0731 12:28:22.133094   37158 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53322/healthz ...
I0731 12:28:22.148949   37158 api_server.go:279] https://127.0.0.1:53322/healthz returned 200:
ok
I0731 12:28:22.155651   37158 api_server.go:141] control plane version: v1.33.1
I0731 12:28:22.155679   37158 api_server.go:131] duration metric: took 9.022975584s to wait for apiserver health ...
I0731 12:28:22.155703   37158 system_pods.go:43] waiting for kube-system pods to appear ...
I0731 12:28:22.170095   37158 system_pods.go:59] 8 kube-system pods found
I0731 12:28:22.170138   37158 system_pods.go:61] "coredns-674b8bbfcf-f4s74" [a7db164c-0459-497e-9e53-264a26a6e50a] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0731 12:28:22.170163   37158 system_pods.go:61] "coredns-674b8bbfcf-znfzf" [6963c676-0319-4f25-b68e-472f1242fa7f] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0731 12:28:22.170169   37158 system_pods.go:61] "etcd-minikube" [b43246f4-c078-4e74-bb6f-b9bf4f952257] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0731 12:28:22.170172   37158 system_pods.go:61] "kube-apiserver-minikube" [13573d26-64f8-492a-8b7e-2b482be91ef0] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0731 12:28:22.170182   37158 system_pods.go:61] "kube-controller-manager-minikube" [b5bd5952-98aa-4754-b097-2c4eba9dc032] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0731 12:28:22.170186   37158 system_pods.go:61] "kube-proxy-ppvxz" [6ad83573-1a41-437a-b236-27c0435580c2] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0731 12:28:22.170189   37158 system_pods.go:61] "kube-scheduler-minikube" [1ff00879-8037-4a45-a40d-2b4008392525] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0731 12:28:22.170191   37158 system_pods.go:61] "storage-provisioner" [dabaebc4-9809-4d4c-a538-3cab3417685b] Running
I0731 12:28:22.170196   37158 system_pods.go:74] duration metric: took 14.487767ms to wait for pod list to return data ...
I0731 12:28:22.170207   37158 kubeadm.go:578] duration metric: took 10.524457013s to wait for: map[apiserver:true system_pods:true]
I0731 12:28:22.170219   37158 node_conditions.go:102] verifying NodePressure condition ...
I0731 12:28:22.236039   37158 node_conditions.go:122] node storage ephemeral capacity is 1055761844Ki
I0731 12:28:22.236100   37158 node_conditions.go:123] node cpu capacity is 8
I0731 12:28:22.237407   37158 node_conditions.go:105] duration metric: took 67.17061ms to run NodePressure ...
I0731 12:28:22.237429   37158 start.go:241] waiting for startup goroutines ...
I0731 12:28:22.646396   37158 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (9.408098745s)
I0731 12:28:22.670990   37158 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0731 12:28:22.710111   37158 addons.go:514] duration metric: took 11.064498003s for enable addons: enabled=[default-storageclass storage-provisioner]
I0731 12:28:22.710161   37158 start.go:246] waiting for cluster config update ...
I0731 12:28:22.710178   37158 start.go:255] writing updated cluster config ...
I0731 12:28:22.711600   37158 ssh_runner.go:195] Run: rm -f paused
I0731 12:28:22.840012   37158 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0731 12:28:22.859756   37158 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 01 17:29:04 minikube cri-dockerd[1381]: time="2025-08-01T17:29:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [==================================>                ]  18.37MB/26.67MB"
Aug 01 17:29:14 minikube cri-dockerd[1381]: time="2025-08-01T17:29:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [==================================>                ]  18.65MB/26.67MB"
Aug 01 17:29:24 minikube cri-dockerd[1381]: time="2025-08-01T17:29:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [==================================>                ]  18.65MB/26.67MB"
Aug 01 17:29:34 minikube cri-dockerd[1381]: time="2025-08-01T17:29:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [===================================>               ]  18.93MB/26.67MB"
Aug 01 17:29:44 minikube cri-dockerd[1381]: time="2025-08-01T17:29:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====================================>              ]  19.21MB/26.67MB"
Aug 01 17:29:54 minikube cri-dockerd[1381]: time="2025-08-01T17:29:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====================================>              ]  19.21MB/26.67MB"
Aug 01 17:30:04 minikube cri-dockerd[1381]: time="2025-08-01T17:30:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====================================>              ]  19.21MB/26.67MB"
Aug 01 17:30:14 minikube cri-dockerd[1381]: time="2025-08-01T17:30:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====================================>              ]  19.21MB/26.67MB"
Aug 01 17:30:24 minikube cri-dockerd[1381]: time="2025-08-01T17:30:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [======================================>            ]   20.6MB/26.67MB"
Aug 01 17:30:27 minikube cri-dockerd[1381]: time="2025-08-01T17:30:27Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Aug 01 17:30:27 minikube dockerd[1044]: time="2025-08-01T17:30:27.597382865Z" level=info msg="ignoring event" container=c6ded65a5c0f6ac69faf766957e44bd7b30bf8de48efc4f306a5ba38977d3a2e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 01 17:30:27 minikube cri-dockerd[1381]: time="2025-08-01T17:30:27Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Aug 01 17:30:28 minikube dockerd[1044]: time="2025-08-01T17:30:28.155694551Z" level=info msg="ignoring event" container=917adecf5678291fad1220c2cc2688f6260e3bdc5f4be8df62ad17dac0d5feb2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 01 17:30:29 minikube dockerd[1044]: time="2025-08-01T17:30:29.836542296Z" level=info msg="ignoring event" container=1d4345fbac44501728a3f8d206eaff3d53f5420c6ee287ea1811b21d1948b553 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 01 17:30:29 minikube dockerd[1044]: time="2025-08-01T17:30:29.882085927Z" level=info msg="ignoring event" container=e6547cdb3401fab738ee220f23e4720106d83fadda653a678484e55d3952f467 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 01 17:30:31 minikube cri-dockerd[1381]: time="2025-08-01T17:30:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/abe102a6045002c11406083111d36bda1f41a63a0b49392f847ce066a55cf2bd/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 01 17:30:47 minikube dockerd[1044]: time="2025-08-01T17:30:47.022244732Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Aug 01 17:30:47 minikube dockerd[1044]: time="2025-08-01T17:30:47.022548506Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Aug 01 17:30:47 minikube dockerd[1044]: time="2025-08-01T17:30:47.028732082Z" level=error msg="Handler for POST /v1.46/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Aug 01 17:31:02 minikube dockerd[1044]: time="2025-08-01T17:31:02.274224097Z" level=warning msg="reference for unknown type: " digest="sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9" remote="registry.k8s.io/ingress-nginx/controller@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9"
Aug 01 17:31:14 minikube cri-dockerd[1381]: time="2025-08-01T17:31:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: c8ce250719f3: Downloading [=========>                                         ]  949.2kB/5.12MB"
Aug 01 17:31:24 minikube cri-dockerd[1381]: time="2025-08-01T17:31:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f18232174bc9: Downloading [====>                                              ]  348.3kB/3.642MB"
Aug 01 17:31:34 minikube cri-dockerd[1381]: time="2025-08-01T17:31:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: c8ce250719f3: Downloading [======================>                            ]   2.32MB/5.12MB"
Aug 01 17:31:44 minikube cri-dockerd[1381]: time="2025-08-01T17:31:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f18232174bc9: Downloading [======>                                            ]  452.8kB/3.642MB"
Aug 01 17:31:54 minikube cri-dockerd[1381]: time="2025-08-01T17:31:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=======>                                           ]  4.844MB/33.65MB"
Aug 01 17:32:04 minikube cri-dockerd[1381]: time="2025-08-01T17:32:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=======>                                           ]  5.188MB/33.65MB"
Aug 01 17:32:14 minikube cri-dockerd[1381]: time="2025-08-01T17:32:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=======>                                           ]  5.188MB/33.65MB"
Aug 01 17:32:24 minikube cri-dockerd[1381]: time="2025-08-01T17:32:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [========>                                          ]  5.532MB/33.65MB"
Aug 01 17:32:34 minikube cri-dockerd[1381]: time="2025-08-01T17:32:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [========>                                          ]  5.876MB/33.65MB"
Aug 01 17:32:44 minikube cri-dockerd[1381]: time="2025-08-01T17:32:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [===========>                                       ]  7.596MB/33.65MB"
Aug 01 17:32:54 minikube cri-dockerd[1381]: time="2025-08-01T17:32:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [======================>                            ]  14.82MB/33.65MB"
Aug 01 17:33:04 minikube cri-dockerd[1381]: time="2025-08-01T17:33:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [===========================>                       ]  18.26MB/33.65MB"
Aug 01 17:33:04 minikube dockerd[1044]: time="2025-08-01T17:33:04.247885283Z" level=info msg="Download failed, retrying (1/5): net/http: TLS handshake timeout"
Aug 01 17:33:14 minikube cri-dockerd[1381]: time="2025-08-01T17:33:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [===========================>                       ]  18.61MB/33.65MB"
Aug 01 17:33:24 minikube cri-dockerd[1381]: time="2025-08-01T17:33:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [============================>                      ]  18.95MB/33.65MB"
Aug 01 17:33:34 minikube cri-dockerd[1381]: time="2025-08-01T17:33:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=================================>                 ]  22.39MB/33.65MB"
Aug 01 17:33:44 minikube cri-dockerd[1381]: time="2025-08-01T17:33:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=======================================>           ]  26.86MB/33.65MB"
Aug 01 17:33:54 minikube cri-dockerd[1381]: time="2025-08-01T17:33:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=======================================>           ]  26.86MB/33.65MB"
Aug 01 17:34:04 minikube cri-dockerd[1381]: time="2025-08-01T17:34:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: e3d2381a937c: Downloading [=>                                                 ]    418kB/20.42MB"
Aug 01 17:34:14 minikube cri-dockerd[1381]: time="2025-08-01T17:34:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [=============================================>     ]  30.65MB/33.65MB"
Aug 01 17:34:15 minikube dockerd[1044]: time="2025-08-01T17:34:15.249532303Z" level=info msg="Download failed, retrying (2/5): net/http: TLS handshake timeout"
Aug 01 17:34:24 minikube cri-dockerd[1381]: time="2025-08-01T17:34:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [===============================================>   ]  31.68MB/33.65MB"
Aug 01 17:34:34 minikube cri-dockerd[1381]: time="2025-08-01T17:34:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: e3d2381a937c: Downloading [==>                                                ]  1.045MB/20.42MB"
Aug 01 17:34:44 minikube cri-dockerd[1381]: time="2025-08-01T17:34:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 5c1d39af7ac1: Downloading [================================================>  ]  32.71MB/33.65MB"
Aug 01 17:34:54 minikube cri-dockerd[1381]: time="2025-08-01T17:34:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: b68d84abb92b: Downloading [>                                                  ]  33.26kB/2.746MB"
Aug 01 17:35:04 minikube cri-dockerd[1381]: time="2025-08-01T17:35:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [=>                                                 ]  444.4kB/21.6MB"
Aug 01 17:35:14 minikube cri-dockerd[1381]: time="2025-08-01T17:35:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [===============================>                   ]  13.62MB/21.6MB"
Aug 01 17:35:24 minikube cri-dockerd[1381]: time="2025-08-01T17:35:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [================================>                  ]  13.85MB/21.6MB"
Aug 01 17:35:34 minikube cri-dockerd[1381]: time="2025-08-01T17:35:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [================================>                  ]  14.08MB/21.6MB"
Aug 01 17:35:44 minikube cri-dockerd[1381]: time="2025-08-01T17:35:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [==================================>                ]     15MB/21.6MB"
Aug 01 17:35:54 minikube cri-dockerd[1381]: time="2025-08-01T17:35:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [===================================>               ]  15.23MB/21.6MB"
Aug 01 17:36:04 minikube cri-dockerd[1381]: time="2025-08-01T17:36:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [===================================>               ]  15.46MB/21.6MB"
Aug 01 17:36:14 minikube cri-dockerd[1381]: time="2025-08-01T17:36:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [====================================>              ]  15.69MB/21.6MB"
Aug 01 17:36:24 minikube cri-dockerd[1381]: time="2025-08-01T17:36:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [====================================>              ]  15.92MB/21.6MB"
Aug 01 17:36:34 minikube cri-dockerd[1381]: time="2025-08-01T17:36:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [=====================================>             ]  16.38MB/21.6MB"
Aug 01 17:36:44 minikube cri-dockerd[1381]: time="2025-08-01T17:36:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [======================================>            ]  16.61MB/21.6MB"
Aug 01 17:36:54 minikube cri-dockerd[1381]: time="2025-08-01T17:36:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [=======================================>           ]  17.06MB/21.6MB"
Aug 01 17:37:04 minikube cri-dockerd[1381]: time="2025-08-01T17:37:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [=========================================>         ]  17.75MB/21.6MB"
Aug 01 17:37:14 minikube cri-dockerd[1381]: time="2025-08-01T17:37:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: 6cbb6654e2c3: Extracting [==================================================>]     215B/215B"
Aug 01 17:37:14 minikube cri-dockerd[1381]: time="2025-08-01T17:37:14Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d173f4a6b42c8       registry.k8s.io/ingress-nginx/controller@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9             3 minutes ago       Running             controller                0                   abe102a604500       ingress-nginx-controller-67c5cb88f-72478
917adecf56782       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524   10 minutes ago      Exited              patch                     0                   1d4345fbac445       ingress-nginx-admission-patch-k2mdg
c6ded65a5c0f6       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524   10 minutes ago      Exited              create                    0                   e6547cdb3401f       ingress-nginx-admission-create-bb549
1bac9d2b6e2cf       6e38f40d628db                                                                                                                11 hours ago        Running             storage-provisioner       10                  43622bff1e607       storage-provisioner
8a0ef4506a5d0       joey727/auth-service@sha256:af1c46fe0aa3a16d1b1cc963f7221ef72d3151693eaa46224628a825bebf4afe                                 29 hours ago        Running             auth-service              0                   c730181e75769       auth-79ff65f6cd-9q4dz
66930571ae154       joey727/auth-service@sha256:af1c46fe0aa3a16d1b1cc963f7221ef72d3151693eaa46224628a825bebf4afe                                 29 hours ago        Running             auth-service              0                   128bfcbe1c596       auth-79ff65f6cd-nckcn
a8b80400e39eb       6e38f40d628db                                                                                                                29 hours ago        Exited              storage-provisioner       9                   43622bff1e607       storage-provisioner
75a5e7954c61b       1cf5f116067c6                                                                                                                29 hours ago        Running             coredns                   4                   f377b7c44e1a1       coredns-674b8bbfcf-znfzf
4c3df2ec12bdc       1cf5f116067c6                                                                                                                29 hours ago        Running             coredns                   4                   9d6ec70f45865       coredns-674b8bbfcf-f4s74
734d2eaa1ff73       b79c189b052cd                                                                                                                29 hours ago        Running             kube-proxy                4                   ef9ce69681420       kube-proxy-ppvxz
18bb3bd09a071       ef43894fa110c                                                                                                                29 hours ago        Running             kube-controller-manager   4                   b3aadaf49f4cc       kube-controller-manager-minikube
a49200fcb52da       c6ab243b29f82                                                                                                                29 hours ago        Running             kube-apiserver            4                   8c600c65dd9cb       kube-apiserver-minikube
8bdfff4bfbb3f       398c985c0d950                                                                                                                29 hours ago        Running             kube-scheduler            4                   c3ea368ae54b9       kube-scheduler-minikube
128747334f5d5       499038711c081                                                                                                                29 hours ago        Running             etcd                      4                   69bd24cac3da0       etcd-minikube
9a3657b78a3bb       1cf5f116067c6                                                                                                                30 hours ago        Exited              coredns                   3                   3fbb97800d07b       coredns-674b8bbfcf-znfzf
9d921ef28a6f9       1cf5f116067c6                                                                                                                30 hours ago        Exited              coredns                   3                   9b0b9335cf9f8       coredns-674b8bbfcf-f4s74
c915e37776079       b79c189b052cd                                                                                                                30 hours ago        Exited              kube-proxy                3                   1abd289bc3f89       kube-proxy-ppvxz
a6f7259503fdc       c6ab243b29f82                                                                                                                30 hours ago        Exited              kube-apiserver            3                   d7701347a7862       kube-apiserver-minikube
ff5154ec11d72       499038711c081                                                                                                                30 hours ago        Exited              etcd                      3                   6463020428b49       etcd-minikube
0f92d304b93b0       398c985c0d950                                                                                                                30 hours ago        Exited              kube-scheduler            3                   62211d993cb4a       kube-scheduler-minikube
5e9d66a760ce0       ef43894fa110c                                                                                                                30 hours ago        Exited              kube-controller-manager   3                   26543eddcba37       kube-controller-manager-minikube


==> coredns [4c3df2ec12bd] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding


==> coredns [75a5e7954c61] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [9a3657b78a3b] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [9d921ef28a6f] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_30T22_20_36_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 30 Jul 2025 22:20:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 01 Aug 2025 17:40:31 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 01 Aug 2025 17:40:01 +0000   Fri, 01 Aug 2025 17:40:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 01 Aug 2025 17:40:01 +0000   Fri, 01 Aug 2025 17:40:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 01 Aug 2025 17:40:01 +0000   Fri, 01 Aug 2025 17:40:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 01 Aug 2025 17:40:01 +0000   Fri, 01 Aug 2025 17:40:01 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055761844Ki
  hugepages-2Mi:      0
  memory:             8027444Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055761844Ki
  hugepages-2Mi:      0
  memory:             8027444Ki
  pods:               110
System Info:
  Machine ID:                 ea4f2560182249b6bb356353eceb0772
  System UUID:                ea4f2560182249b6bb356353eceb0772
  Boot ID:                    2828da37-cdb5-4f52-b625-220bab398f8f
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     auth-79ff65f6cd-9q4dz                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         29h
  default                     auth-79ff65f6cd-nckcn                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         29h
  ingress-nginx               ingress-nginx-controller-67c5cb88f-72478    100m (1%)     0 (0%)      90Mi (1%)        0 (0%)         16m
  kube-system                 coredns-674b8bbfcf-f4s74                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     43h
  kube-system                 coredns-674b8bbfcf-znfzf                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     43h
  kube-system                 etcd-minikube                               100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         43h
  kube-system                 kube-apiserver-minikube                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         43h
  kube-system                 kube-controller-manager-minikube            200m (2%)     0 (0%)      0 (0%)           0 (0%)         43h
  kube-system                 kube-proxy-ppvxz                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         43h
  kube-system                 kube-scheduler-minikube                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         43h
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         43h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (11%)  0 (0%)
  memory             330Mi (4%)  340Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                 From             Message
  ----    ------                   ----                ----             -------
  Normal  NodeNotReady             41s (x6 over 28h)   node-controller  Node minikube status is now: NodeNotReady
  Normal  NodeNotReady             41s (x11 over 28h)  kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeHasSufficientMemory  30s (x34 over 29h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    30s (x34 over 29h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     30s (x34 over 29h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                30s (x12 over 28h)  kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[  +0.000002] FS:  0000000000000000(0000) GS:ffff9be3f1940000(0000) knlGS:0000000000000000
[  +0.000001] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000002] CR2: 00007f9ef9f77ab0 CR3: 000000018573a006 CR4: 0000000000370eb0
[  +0.000001] Call Trace:
[  +0.000003]  <IRQ>
[  +0.000004]  ? rcu_dump_cpu_stacks+0xcb/0x110
[  +0.000005]  ? rcu_sched_clock_irq+0x347/0x10e0
[  +0.000003]  ? local_clock_noinstr+0xd/0xc0
[  +0.000009]  ? __pfx_hrtimer_wakeup+0x10/0x10
[  +0.000004]  ? hrtimer_wakeup+0x22/0x30
[  +0.000030]  ? __hrtimer_run_queues+0x121/0x2a0
[  +0.000007]  ? update_process_times+0x6c/0xb0
[  +0.000003]  ? tick_nohz_lowres_handler+0xb8/0x170
[  +0.000004]  ? __sysvec_apic_timer_interrupt+0x58/0x150
[  +0.000005]  ? sysvec_apic_timer_interrupt+0x64/0x80
[  +0.000006]  </IRQ>
[  +0.000001]  <TASK>
[  +0.000001]  ? asm_sysvec_apic_timer_interrupt+0x1a/0x20
[  +0.000006]  ? default_idle+0xf/0x20
[  +0.000002]  default_idle_call+0x30/0xf0
[  +0.000003]  do_idle+0x1b0/0x1c0
[  +0.000005]  cpu_startup_entry+0x29/0x30
[  +0.000003]  start_secondary+0xf5/0x100
[  +0.000003]  common_startup_64+0x13b/0x148
[  +0.000005]  </TASK>
[Jul31 15:54] Hangcheck: hangcheck value past margin!
[Jul31 16:10] Hangcheck: hangcheck value past margin!
[Jul31 16:26] Hangcheck: hangcheck value past margin!
[Jul31 16:43] Hangcheck: hangcheck value past margin!
[Jul31 16:47] systemd-journald[38282]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jul31 17:05] Hangcheck: hangcheck value past margin!
[  +0.464959] systemd-journald[38885]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jul31 17:40] Hangcheck: hangcheck value past margin!
[Jul31 17:58] Hangcheck: hangcheck value past margin!
[Jul31 18:06] Hangcheck: hangcheck value past margin!
[Jul31 18:15] Hangcheck: hangcheck value past margin!
[Jul31 18:38] systemd-journald[42807]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jul31 21:15] Hangcheck: hangcheck value past margin!
[  +0.203679] systemd-journald[73281]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jul31 22:01] Hangcheck: hangcheck value past margin!
[Jul31 22:27] Hangcheck: hangcheck value past margin!
[Aug 1 05:04] Hangcheck: hangcheck value past margin!
[Aug 1 06:16] Hangcheck: hangcheck value past margin!
[Aug 1 06:36] Hangcheck: hangcheck value past margin!
[  +1.973646] systemd-journald[128239]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Aug 1 07:08] Hangcheck: hangcheck value past margin!
[Aug 1 08:05] Hangcheck: hangcheck value past margin!
[Aug 1 08:30] Hangcheck: hangcheck value past margin!
[Aug 1 09:09] Hangcheck: hangcheck value past margin!
[Aug 1 10:10] Hangcheck: hangcheck value past margin!
[Aug 1 11:11] Hangcheck: hangcheck value past margin!
[Aug 1 12:12] Hangcheck: hangcheck value past margin!
[Aug 1 13:13] Hangcheck: hangcheck value past margin!
[Aug 1 14:14] Hangcheck: hangcheck value past margin!
[Aug 1 14:18] Hangcheck: hangcheck value past margin!
[Aug 1 15:28] Hangcheck: hangcheck value past margin!
[Aug 1 15:29] systemd-journald[130957]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Aug 1 16:40] Hangcheck: hangcheck value past margin!
[Aug 1 16:45] Hangcheck: hangcheck value past margin!
[Aug 1 17:39] systemd-journald[146733]: File /run/log/journal/ea4f2560182249b6bb356353eceb0772/system.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [128747334f5d] <==
{"level":"info","ts":"2025-08-01T17:39:50.262915Z","caller":"traceutil/trace.go:171","msg":"trace[490108368] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:40777; }","duration":"494.676283ms","start":"2025-08-01T17:39:49.786104Z","end":"2025-08-01T17:39:50.262907Z","steps":["trace[490108368] 'agreement among raft nodes before linearized reading'  (duration: 494.622827ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.262931Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.784454Z","time spent":"496.345923ms","remote":"127.0.0.1:54418","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":70,"response size":32,"request content":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.263018Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"494.785224ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.263035Z","caller":"traceutil/trace.go:171","msg":"trace[743412551] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:40777; }","duration":"494.807794ms","start":"2025-08-01T17:39:49.786095Z","end":"2025-08-01T17:39:50.263030Z","steps":["trace[743412551] 'agreement among raft nodes before linearized reading'  (duration: 494.780494ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.263049Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.784450Z","time spent":"496.468594ms","remote":"127.0.0.1:54194","response type":"/etcdserverpb.KV/Range","request count":0,"request size":72,"response count":0,"response size":30,"request content":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.263132Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"494.908326ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.263149Z","caller":"traceutil/trace.go:171","msg":"trace[1894460066] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:40777; }","duration":"494.930928ms","start":"2025-08-01T17:39:49.786087Z","end":"2025-08-01T17:39:50.263144Z","steps":["trace[1894460066] 'agreement among raft nodes before linearized reading'  (duration: 494.899444ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.263164Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.784446Z","time spent":"496.588045ms","remote":"127.0.0.1:54396","response type":"/etcdserverpb.KV/Range","request count":0,"request size":38,"response count":14,"response size":32,"request content":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.263247Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"495.032242ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.263266Z","caller":"traceutil/trace.go:171","msg":"trace[793963643] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:40777; }","duration":"495.05852ms","start":"2025-08-01T17:39:49.786076Z","end":"2025-08-01T17:39:50.263261Z","steps":["trace[793963643] 'agreement among raft nodes before linearized reading'  (duration: 495.026385ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.263282Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.784440Z","time spent":"496.710702ms","remote":"127.0.0.1:54268","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":30,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.263897Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"496.003939ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.263957Z","caller":"traceutil/trace.go:171","msg":"trace[571671019] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:40777; }","duration":"496.07653ms","start":"2025-08-01T17:39:49.785743Z","end":"2025-08-01T17:39:50.263946Z","steps":["trace[571671019] 'agreement among raft nodes before linearized reading'  (duration: 495.971141ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.263998Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.777103Z","time spent":"504.759486ms","remote":"127.0.0.1:44576","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/pods\" limit:1 "}
{"level":"warn","ts":"2025-08-01T17:39:50.265536Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"497.624991ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" limit:1 ","response":"range_response_count:1 size:503"}
{"level":"info","ts":"2025-08-01T17:39:50.265604Z","caller":"traceutil/trace.go:171","msg":"trace[125350141] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:40777; }","duration":"497.722231ms","start":"2025-08-01T17:39:49.785733Z","end":"2025-08-01T17:39:50.265582Z","steps":["trace[125350141] 'agreement among raft nodes before linearized reading'  (duration: 497.201605ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.265645Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.776997Z","time spent":"506.509601ms","remote":"127.0.0.1:54310","response type":"/etcdserverpb.KV/Range","request count":0,"request size":55,"response count":1,"response size":527,"request content":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" limit:1 "}
{"level":"warn","ts":"2025-08-01T17:39:50.265885Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.014261ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.265919Z","caller":"traceutil/trace.go:171","msg":"trace[249701297] range","detail":"{range_begin:/registry/controllerrevisions/; range_end:/registry/controllerrevisions0; response_count:0; response_revision:40777; }","duration":"498.057109ms","start":"2025-08-01T17:39:49.785724Z","end":"2025-08-01T17:39:50.265908Z","steps":["trace[249701297] 'agreement among raft nodes before linearized reading'  (duration: 497.994792ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.265947Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.776878Z","time spent":"506.936298ms","remote":"127.0.0.1:54522","response type":"/etcdserverpb.KV/Range","request count":0,"request size":66,"response count":1,"response size":32,"request content":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.266065Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.210229ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.266085Z","caller":"traceutil/trace.go:171","msg":"trace[1359168611] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:40777; }","duration":"498.238809ms","start":"2025-08-01T17:39:49.785713Z","end":"2025-08-01T17:39:50.266079Z","steps":["trace[1359168611] 'agreement among raft nodes before linearized reading'  (duration: 498.201967ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.266105Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.776804Z","time spent":"507.166977ms","remote":"127.0.0.1:54190","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":0,"response size":30,"request content":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.266207Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.364955ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/node-controller\" limit:1 ","response":"range_response_count:1 size:196"}
{"level":"info","ts":"2025-08-01T17:39:50.266224Z","caller":"traceutil/trace.go:171","msg":"trace[967037665] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/node-controller; range_end:; response_count:1; response_revision:40777; }","duration":"498.387972ms","start":"2025-08-01T17:39:49.785704Z","end":"2025-08-01T17:39:50.266219Z","steps":["trace[967037665] 'agreement among raft nodes before linearized reading'  (duration: 498.348409ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.266239Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.776409Z","time spent":"507.699429ms","remote":"127.0.0.1:54250","response type":"/etcdserverpb.KV/Range","request count":0,"request size":57,"response count":1,"response size":220,"request content":"key:\"/registry/serviceaccounts/kube-system/node-controller\" limit:1 "}
{"level":"warn","ts":"2025-08-01T17:39:50.266323Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.490414ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.266344Z","caller":"traceutil/trace.go:171","msg":"trace[380286249] range","detail":"{range_begin:/registry/serviceaccounts/; range_end:/registry/serviceaccounts0; response_count:0; response_revision:40777; }","duration":"498.508676ms","start":"2025-08-01T17:39:49.785701Z","end":"2025-08-01T17:39:50.266336Z","steps":["trace[380286249] 'agreement among raft nodes before linearized reading'  (duration: 498.472986ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.266364Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.776393Z","time spent":"507.839579ms","remote":"127.0.0.1:54250","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":44,"response size":32,"request content":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.266473Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.643075ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2025-08-01T17:39:50.266496Z","caller":"traceutil/trace.go:171","msg":"trace[453597297] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:40777; }","duration":"498.665319ms","start":"2025-08-01T17:39:49.785696Z","end":"2025-08-01T17:39:50.266488Z","steps":["trace[453597297] 'agreement among raft nodes before linearized reading'  (duration: 498.624406ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.266523Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.771982Z","time spent":"512.399725ms","remote":"127.0.0.1:54210","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-08-01T17:39:50.266648Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.830581ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.266675Z","caller":"traceutil/trace.go:171","msg":"trace[1743931398] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:40777; }","duration":"498.85926ms","start":"2025-08-01T17:39:49.785684Z","end":"2025-08-01T17:39:50.266670Z","steps":["trace[1743931398] 'agreement among raft nodes before linearized reading'  (duration: 498.812666ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.266694Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.771851Z","time spent":"512.710004ms","remote":"127.0.0.1:54438","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":2,"response size":32,"request content":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.266772Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"498.960795ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.266785Z","caller":"traceutil/trace.go:171","msg":"trace[1028541436] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:0; response_revision:40777; }","duration":"498.980351ms","start":"2025-08-01T17:39:49.785674Z","end":"2025-08-01T17:39:50.266781Z","steps":["trace[1028541436] 'agreement among raft nodes before linearized reading'  (duration: 498.957595ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.266800Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.771766Z","time spent":"512.90392ms","remote":"127.0.0.1:44580","response type":"/etcdserverpb.KV/Range","request count":0,"request size":39,"response count":0,"response size":30,"request content":"key:\"/registry/masterleases/192.168.49.2\" limit:1 "}
{"level":"warn","ts":"2025-08-01T17:39:50.266835Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"499.027695ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.266880Z","caller":"traceutil/trace.go:171","msg":"trace[215711280] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:40777; }","duration":"499.082836ms","start":"2025-08-01T17:39:49.785663Z","end":"2025-08-01T17:39:50.266873Z","steps":["trace[215711280] 'agreement among raft nodes before linearized reading'  (duration: 499.019309ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.267655Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"499.853244ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.267694Z","caller":"traceutil/trace.go:171","msg":"trace[1707261761] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:40777; }","duration":"499.904232ms","start":"2025-08-01T17:39:49.785653Z","end":"2025-08-01T17:39:50.267684Z","steps":["trace[1707261761] 'agreement among raft nodes before linearized reading'  (duration: 499.800312ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.267736Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.771632Z","time spent":"513.957885ms","remote":"127.0.0.1:37188","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":50,"response size":32,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.268124Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"500.336536ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/servicecidrs/\" range_end:\"/registry/servicecidrs0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.268155Z","caller":"traceutil/trace.go:171","msg":"trace[401529113] range","detail":"{range_begin:/registry/servicecidrs/; range_end:/registry/servicecidrs0; response_count:0; response_revision:40777; }","duration":"500.377759ms","start":"2025-08-01T17:39:49.785642Z","end":"2025-08-01T17:39:50.268147Z","steps":["trace[401529113] 'agreement among raft nodes before linearized reading'  (duration: 500.319886ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.268177Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.771561Z","time spent":"514.483564ms","remote":"127.0.0.1:54370","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":32,"request content":"key:\"/registry/servicecidrs/\" range_end:\"/registry/servicecidrs0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.269155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"500.966134ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.269198Z","caller":"traceutil/trace.go:171","msg":"trace[994914813] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:40777; }","duration":"501.021773ms","start":"2025-08-01T17:39:49.786038Z","end":"2025-08-01T17:39:50.269187Z","steps":["trace[994914813] 'agreement among raft nodes before linearized reading'  (duration: 500.926142ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.269240Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.783934Z","time spent":"503.168724ms","remote":"127.0.0.1:60124","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":50,"response size":32,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.269502Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"501.301273ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.269536Z","caller":"traceutil/trace.go:171","msg":"trace[717299480] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:40777; }","duration":"501.343038ms","start":"2025-08-01T17:39:49.786058Z","end":"2025-08-01T17:39:50.269528Z","steps":["trace[717299480] 'agreement among raft nodes before linearized reading'  (duration: 501.280176ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.269559Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.783960Z","time spent":"503.466133ms","remote":"127.0.0.1:54404","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":14,"response size":32,"request content":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.269815Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"501.624504ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-08-01T17:39:50.269842Z","caller":"traceutil/trace.go:171","msg":"trace[362334564] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:40777; }","duration":"501.654328ms","start":"2025-08-01T17:39:49.786054Z","end":"2025-08-01T17:39:50.269835Z","steps":["trace[362334564] 'agreement among raft nodes before linearized reading'  (duration: 501.603069ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.269863Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.783952Z","time spent":"503.778208ms","remote":"127.0.0.1:54450","response type":"/etcdserverpb.KV/Range","request count":0,"request size":44,"response count":1,"response size":32,"request content":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.270224Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"502.049833ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-08-01T17:39:50.270253Z","caller":"traceutil/trace.go:171","msg":"trace[707228458] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:40777; }","duration":"502.089077ms","start":"2025-08-01T17:39:49.786030Z","end":"2025-08-01T17:39:50.270246Z","steps":["trace[707228458] 'agreement among raft nodes before linearized reading'  (duration: 502.038151ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-01T17:39:50.270274Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.783906Z","time spent":"504.236408ms","remote":"127.0.0.1:54280","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":30,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.278078Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.771708Z","time spent":"513.06002ms","remote":"127.0.0.1:54168","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":30,"request content":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true "}
{"level":"warn","ts":"2025-08-01T17:39:50.366701Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-01T17:39:49.783885Z","time spent":"410.012983ms","remote":"127.0.0.1:44576","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":13,"response size":32,"request content":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true "}


==> etcd [ff5154ec11d7] <==
{"level":"info","ts":"2025-07-31T11:37:45.530315Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-31T11:37:45.530563Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-31T11:37:45.530575Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-31T11:37:45.530488Z","caller":"etcdserver/server.go:775","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-07-31T11:37:45.533932Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-31T11:37:45.534614Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-31T11:37:45.534783Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-31T11:37:45.535734Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-31T11:37:45.536097Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-31T11:37:45.537455Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-07-31T11:37:45.538107Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-31T11:37:45.538434Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-31T11:37:45.538471Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-31T11:37:47.328915Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2025-07-31T11:37:47.329068Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2025-07-31T11:37:47.329101Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-07-31T11:37:47.329121Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2025-07-31T11:37:47.329160Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-07-31T11:37:47.329175Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2025-07-31T11:37:47.329188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-07-31T11:37:47.335061Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-31T11:37:47.335166Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-31T11:37:47.335282Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-31T11:37:47.336944Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-31T11:37:47.337104Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-31T11:37:47.337376Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-31T11:37:47.337063Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-31T11:37:47.339194Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-31T11:37:47.339439Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-31T11:47:47.365765Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6406}
{"level":"info","ts":"2025-07-31T11:47:47.412094Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":6406,"took":"45.64335ms","hash":4273183713,"current-db-size-bytes":5251072,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":2076672,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-07-31T11:47:47.412199Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4273183713,"revision":6406,"compact-revision":4453}
{"level":"info","ts":"2025-07-31T11:52:47.306552Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6753}
{"level":"info","ts":"2025-07-31T11:52:47.310699Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":6753,"took":"3.792138ms","hash":2480574144,"current-db-size-bytes":5251072,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":2408448,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-07-31T11:52:47.310771Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2480574144,"revision":6753,"compact-revision":6406}
{"level":"info","ts":"2025-07-31T11:57:47.318050Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7127}
{"level":"info","ts":"2025-07-31T11:57:47.339535Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7127,"took":"20.449313ms","hash":4019533017,"current-db-size-bytes":5251072,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":1687552,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-31T11:57:47.339691Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4019533017,"revision":7127,"compact-revision":6753}
{"level":"info","ts":"2025-07-31T12:02:47.321219Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7473}
{"level":"info","ts":"2025-07-31T12:02:47.326430Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7473,"took":"4.800039ms","hash":2911132810,"current-db-size-bytes":5251072,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":2252800,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-07-31T12:02:47.326495Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2911132810,"revision":7473,"compact-revision":7127}
{"level":"info","ts":"2025-07-31T12:07:47.360503Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7712}
{"level":"info","ts":"2025-07-31T12:07:47.366542Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7712,"took":"5.264908ms","hash":1284117578,"current-db-size-bytes":5251072,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":2379776,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-07-31T12:07:47.366658Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1284117578,"revision":7712,"compact-revision":7473}
{"level":"info","ts":"2025-07-31T12:11:43.027034Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-07-31T12:11:43.034070Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2025-07-31T12:11:43.034169Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2025-07-31T12:12:47.375161Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8099}
{"level":"info","ts":"2025-07-31T12:12:47.396159Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8099,"took":"20.211695ms","hash":2000750125,"current-db-size-bytes":5251072,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-31T12:12:47.396294Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2000750125,"revision":8099,"compact-revision":7712}
{"level":"info","ts":"2025-07-31T12:15:04.562297Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-07-31T12:15:04.562363Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-07-31T12:15:11.567326Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2025-07-31T12:15:11.567367Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-31T12:15:11.567439Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-31T12:15:11.567513Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-31T12:15:11.567557Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-07-31T12:15:11.576115Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-31T12:15:11.576459Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-31T12:15:11.576484Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 17:40:32 up 1 day,  6:47,  0 users,  load average: 1.72, 2.26, 2.64
Linux minikube 6.10.14-linuxkit #1 SMP PREEMPT_DYNAMIC Tue Apr 15 16:05:22 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a49200fcb52d] <==
E0801 00:21:53.160812       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="297.993841ms" method="PATCH" path="/api/v1/namespaces/default/events/minikube.1857572794c1fbb0" result=null
E0801 00:21:53.341536       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0801 00:21:53.342802       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0801 00:21:53.348714       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="990.306752ms" method="PATCH" path="/api/v1/nodes/minikube/status" result=null
E0801 00:21:53.458227       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 316.244184ms, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E0801 00:22:03.869384       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: e3019b9f-2c7a-406c-a69f-bb5c8c111aee, UID in object meta: "
I0801 00:31:52.665553       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 00:41:52.677185       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 00:51:52.690549       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 01:01:52.703836       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 01:11:52.719120       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 01:21:52.557548       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 01:31:52.569007       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 01:41:52.550217       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 01:51:52.546136       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 02:01:52.553431       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 02:11:52.532216       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 02:21:52.522768       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 02:31:52.530077       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 02:41:52.315950       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 02:51:52.271422       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 03:01:52.261295       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 03:11:52.322386       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 03:21:52.245110       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 03:31:52.236916       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 03:41:52.212201       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 03:51:52.272742       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 04:01:52.268805       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 04:11:52.211247       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 04:21:52.186003       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 04:31:52.176147       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 05:04:39.220230       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 06:16:14.680092       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0801 06:16:26.879250       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 3885ed81-3de3-4674-a068-d897e2f197e1, UID in object meta: "
I0801 06:36:02.089897       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0801 06:36:03.131630       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 85.52773ms, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E0801 06:36:03.429278       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0801 06:36:03.546144       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0801 06:36:03.622167       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0801 06:36:03.623844       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="279.474333ms" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
I0801 15:05:43.533404       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 15:05:45.031380       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 15:05:45.128550       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0801 15:05:54.571426       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 4bed036e-67ed-4f18-920b-32169be02a42, UID in object meta: "
I0801 15:15:43.572037       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 15:29:01.125633       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 15:39:01.203148       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 16:40:58.961238       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 16:50:58.973572       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 17:00:58.965650       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 17:10:58.965848       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 17:20:58.895512       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 17:24:19.066376       1 controller.go:667] quota admission added evaluator for: namespaces
I0801 17:24:19.102669       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0801 17:24:19.136558       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0801 17:24:19.227958       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0801 17:24:19.227280       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.101.19.57"}
I0801 17:24:19.252544       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.99.225.240"}
I0801 17:24:19.296348       1 controller.go:667] quota admission added evaluator for: jobs.batch
I0801 17:30:58.887513       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-apiserver [a6f7259503fd] <==
W0731 12:15:10.278390       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:10.294152       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:10.307691       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:10.312925       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:10.312951       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:10.324024       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:12.552504       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:12.818419       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:12.856215       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:12.900889       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.024425       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.155964       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.169297       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.230821       1 logging.go:55] [core] [Channel #3 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.292149       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.303212       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.313915       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.318995       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.391106       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.391972       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.442100       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.452209       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.452274       1 logging.go:55] [core] [Channel #8 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.481656       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.496119       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.509049       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.510805       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.546750       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.563705       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.589957       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.608996       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.634005       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.638478       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.649710       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.705723       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.737163       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.765175       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.793073       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.871998       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.886135       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.946581       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.952384       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:13.970766       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.017041       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.024731       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.025108       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.044214       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.052390       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.085642       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.195963       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.217902       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.246424       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.307983       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.354846       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.410919       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.415834       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.440733       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.484973       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.515456       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0731 12:15:14.540497       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [18bb3bd09a07] <==
I0731 12:28:23.623491       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0731 12:28:23.623676       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0731 12:28:23.626463       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0731 12:28:23.624320       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0731 12:28:23.624382       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0731 12:28:23.627258       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0731 12:28:23.624429       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0731 12:28:23.624258       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0731 12:28:23.624468       1 shared_informer.go:357] "Caches are synced" controller="job"
I0731 12:28:23.627430       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0731 12:28:23.630836       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0731 12:28:23.632938       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0731 12:28:23.637061       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0731 12:28:23.637166       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0731 12:28:23.638407       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0731 12:28:23.639584       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0731 12:28:23.639612       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0731 12:28:23.643816       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0731 12:28:23.649721       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0731 12:28:23.651146       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0731 12:28:23.652337       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0731 12:28:23.722364       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0731 12:28:23.722474       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0731 12:28:23.722742       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0731 12:28:23.729458       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0731 12:28:24.107960       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0731 12:28:24.108080       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0731 12:28:24.108101       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0731 12:28:24.135306       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0731 12:53:17.444254       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0731 12:53:27.476039       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0731 13:28:22.977528       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-vdqpn" approvedExpiration="1h0m0s"
I0731 15:23:07.660600       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0731 15:23:22.734399       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0731 16:47:15.813330       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="Get \"https://192.168.49.2:8443/api\": dial tcp 192.168.49.2:8443: i/o timeout"
E0731 16:47:17.034789       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0731 16:47:17.213097       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0731 16:47:32.219064       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0731 17:05:46.224236       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="EndpointSlice informer cache is out of date"
I0731 17:05:46.440233       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0731 17:06:01.524457       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
E0731 18:38:09.272346       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0731 18:38:09.371282       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0731 18:38:24.375325       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0731 21:15:08.747466       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0801 00:22:07.364598       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
E0801 05:04:40.096508       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0801 05:04:40.125232       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
E0801 06:16:15.279790       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0801 06:16:41.089460       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
E0801 06:36:03.843372       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0801 06:36:04.019394       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0801 15:05:59.933321       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0801 15:29:03.121712       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0801 15:29:13.123846       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0801 16:41:00.510899       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0801 16:41:10.512802       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0801 17:39:51.168794       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="EndpointSlice informer cache is out of date"
I0801 17:39:51.248760       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0801 17:40:01.251601       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"


==> kube-controller-manager [5e9d66a760ce] <==
I0731 11:37:54.826763       1 shared_informer.go:350] "Waiting for caches to sync" controller="persistent volume"
I0731 11:37:54.846021       1 controllermanager.go:778] "Started controller" controller="root-ca-certificate-publisher-controller"
I0731 11:37:54.846074       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I0731 11:37:54.847079       1 shared_informer.go:350] "Waiting for caches to sync" controller="crt configmap"
I0731 11:37:55.054118       1 controllermanager.go:778] "Started controller" controller="validatingadmissionpolicy-status-controller"
I0731 11:37:55.120610       1 shared_informer.go:350] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
I0731 11:37:55.131153       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0731 11:37:55.132564       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0731 11:37:55.143748       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0731 11:37:55.221052       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0731 11:37:55.221108       1 shared_informer.go:357] "Caches are synced" controller="node"
I0731 11:37:55.221894       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0731 11:37:55.222188       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0731 11:37:55.222244       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0731 11:37:55.222277       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0731 11:37:55.225517       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0731 11:37:55.225686       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0731 11:37:55.226101       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0731 11:37:55.226492       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0731 11:37:55.227244       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0731 11:37:55.232374       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0731 11:37:55.235560       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0731 11:37:55.237850       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0731 11:37:55.238113       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0731 11:37:55.240403       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0731 11:37:55.240918       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0731 11:37:55.241998       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0731 11:37:55.242993       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0731 11:37:55.247258       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0731 11:37:55.320216       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0731 11:37:55.321221       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0731 11:37:55.326622       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0731 11:37:55.327692       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0731 11:37:55.337561       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0731 11:37:55.337897       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0731 11:37:55.327741       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0731 11:37:55.327996       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0731 11:37:55.341041       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0731 11:37:55.341068       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0731 11:37:55.341855       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0731 11:37:55.341934       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0731 11:37:55.344549       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0731 11:37:55.422283       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0731 11:37:55.427844       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0731 11:37:55.432171       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0731 11:37:55.434896       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0731 11:37:55.445680       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0731 11:37:55.446060       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0731 11:37:55.523162       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0731 11:37:55.524042       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0731 11:37:55.528935       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0731 11:37:55.530799       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0731 11:37:55.531157       1 shared_informer.go:357] "Caches are synced" controller="job"
I0731 11:37:55.531327       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0731 11:37:55.545167       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0731 11:37:55.948261       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0731 11:37:56.039876       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0731 11:37:56.039982       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0731 11:37:56.040000       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0731 11:53:46.360988       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/auth-service" err="EndpointSlice informer cache is out of date"


==> kube-proxy [734d2eaa1ff7] <==
I0731 12:28:21.957522       1 server_linux.go:63] "Using iptables proxy"
I0731 12:28:22.541434       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0731 12:28:22.541858       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0731 12:28:22.825868       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0731 12:28:22.825937       1 server_linux.go:145] "Using iptables Proxier"
I0731 12:28:22.838073       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0731 12:28:22.838899       1 server.go:516] "Version info" version="v1.33.1"
I0731 12:28:22.839413       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 12:28:22.842780       1 config.go:199] "Starting service config controller"
I0731 12:28:22.842853       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0731 12:28:22.842883       1 config.go:329] "Starting node config controller"
I0731 12:28:22.842887       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0731 12:28:22.843163       1 config.go:105] "Starting endpoint slice config controller"
I0731 12:28:22.845751       1 config.go:440] "Starting serviceCIDR config controller"
I0731 12:28:22.846083       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0731 12:28:22.846143       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0731 12:28:22.849236       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0731 12:28:22.944211       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0731 12:28:22.944349       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0731 12:28:22.949929       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [c915e3777607] <==
I0731 11:37:54.238151       1 server_linux.go:63] "Using iptables proxy"
I0731 11:37:54.722249       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0731 11:37:54.722470       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0731 11:37:55.034156       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0731 11:37:55.034359       1 server_linux.go:145] "Using iptables Proxier"
I0731 11:37:55.055081       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0731 11:37:55.055918       1 server.go:516] "Version info" version="v1.33.1"
I0731 11:37:55.057896       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 11:37:55.064600       1 config.go:199] "Starting service config controller"
I0731 11:37:55.064622       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0731 11:37:55.064649       1 config.go:105] "Starting endpoint slice config controller"
I0731 11:37:55.064653       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0731 11:37:55.064671       1 config.go:440] "Starting serviceCIDR config controller"
I0731 11:37:55.064675       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0731 11:37:55.065318       1 config.go:329] "Starting node config controller"
I0731 11:37:55.065324       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0731 11:37:55.220799       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0731 11:37:55.221148       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0731 11:37:55.221165       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0731 11:37:55.221296       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [0f92d304b93b] <==
I0731 11:37:45.944304       1 serving.go:386] Generated self-signed cert in-memory
W0731 11:37:49.621421       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0731 11:37:49.621541       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0731 11:37:49.621564       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0731 11:37:49.621583       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0731 11:37:49.933922       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0731 11:37:49.933983       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 11:37:49.940226       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0731 11:37:49.940414       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0731 11:37:49.940442       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0731 11:37:49.940467       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0731 11:37:50.043381       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0731 12:15:04.564117       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [8bdfff4bfbb3] <==
I0731 12:28:13.656791       1 serving.go:386] Generated self-signed cert in-memory
W0731 12:28:17.037090       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0731 12:28:17.037140       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0731 12:28:17.037157       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0731 12:28:17.037167       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0731 12:28:17.245890       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0731 12:28:17.245955       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 12:28:17.337208       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0731 12:28:17.337925       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0731 12:28:17.338816       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0731 12:28:17.338013       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0731 12:28:17.444999       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0801 00:21:52.439684       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.440871       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.442315       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.442439       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.442620       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.442985       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.443347       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.442932       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.444525       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.444756       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.444820       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.445102       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.445144       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.445296       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.445555       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 00:21:52.545817       1 reflector.go:556] "Warning: watch ended with error" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0801 15:05:43.531094       1 reflector.go:556] "Warning: watch ended with error" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"


==> kubelet <==
Aug 01 15:29:01 minikube kubelet[1603]: E0801 15:29:01.526346    1603 kubelet.go:2460] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 11m32.112422827s ago; threshold is 3m0s]"
Aug 01 15:29:01 minikube kubelet[1603]: I0801 15:29:01.705541    1603 setters.go:618] "Node became not ready" node="minikube" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2025-08-01T15:29:01Z","lastTransitionTime":"2025-08-01T15:29:01Z","reason":"KubeletNotReady","message":"[container runtime is down, PLEG is not healthy: pleg was last seen active 11m32.291590031s ago; threshold is 3m0s]"}
Aug 01 15:29:01 minikube kubelet[1603]: E0801 15:29:01.727122    1603 kubelet.go:2460] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 11m32.313198897s ago; threshold is 3m0s]"
Aug 01 16:40:58 minikube kubelet[1603]: E0801 16:40:58.969422    1603 kubelet.go:2460] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 1h0m56.606807466s ago; threshold is 3m0s]"
Aug 01 16:40:59 minikube kubelet[1603]: E0801 16:40:59.156277    1603 kubelet.go:2460] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 1h0m56.793656104s ago; threshold is 3m0s]"
Aug 01 16:40:59 minikube kubelet[1603]: E0801 16:40:59.357591    1603 kubelet.go:2460] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 1h0m56.994976033s ago; threshold is 3m0s]"
Aug 01 16:40:59 minikube kubelet[1603]: E0801 16:40:59.760427    1603 kubelet.go:2460] "Skipping pod synchronization" err="container runtime is down"
Aug 01 16:40:59 minikube kubelet[1603]: I0801 16:40:59.762763    1603 setters.go:618] "Node became not ready" node="minikube" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2025-08-01T16:40:59Z","lastTransitionTime":"2025-08-01T16:40:59Z","reason":"KubeletNotReady","message":"container runtime is down"}
Aug 01 17:24:19 minikube kubelet[1603]: I0801 17:24:19.586083    1603 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert\") pod \"ingress-nginx-controller-67c5cb88f-72478\" (UID: \"c4b6e9cf-14ac-41be-91fe-7be0f01d24db\") " pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478"
Aug 01 17:24:19 minikube kubelet[1603]: I0801 17:24:19.586180    1603 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nj8lw\" (UniqueName: \"kubernetes.io/projected/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-kube-api-access-nj8lw\") pod \"ingress-nginx-controller-67c5cb88f-72478\" (UID: \"c4b6e9cf-14ac-41be-91fe-7be0f01d24db\") " pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478"
Aug 01 17:24:19 minikube kubelet[1603]: I0801 17:24:19.686494    1603 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2d7gb\" (UniqueName: \"kubernetes.io/projected/b868a4e3-d8cc-443f-b47b-909407dffa9c-kube-api-access-2d7gb\") pod \"ingress-nginx-admission-patch-k2mdg\" (UID: \"b868a4e3-d8cc-443f-b47b-909407dffa9c\") " pod="ingress-nginx/ingress-nginx-admission-patch-k2mdg"
Aug 01 17:24:19 minikube kubelet[1603]: I0801 17:24:19.686634    1603 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zm99z\" (UniqueName: \"kubernetes.io/projected/91494c56-487e-4368-8ce1-c5d8ad34c2dd-kube-api-access-zm99z\") pod \"ingress-nginx-admission-create-bb549\" (UID: \"91494c56-487e-4368-8ce1-c5d8ad34c2dd\") " pod="ingress-nginx/ingress-nginx-admission-create-bb549"
Aug 01 17:24:19 minikube kubelet[1603]: E0801 17:24:19.689957    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:19 minikube kubelet[1603]: E0801 17:24:19.692493    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:24:20.190925859 +0000 UTC m=+104171.811361132 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:24:20 minikube kubelet[1603]: E0801 17:24:20.196283    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:20 minikube kubelet[1603]: E0801 17:24:20.196683    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:24:21.196648415 +0000 UTC m=+104172.817083692 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:24:20 minikube kubelet[1603]: I0801 17:24:20.213226    1603 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1d4345fbac44501728a3f8d206eaff3d53f5420c6ee287ea1811b21d1948b553"
Aug 01 17:24:20 minikube kubelet[1603]: I0801 17:24:20.218712    1603 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e6547cdb3401fab738ee220f23e4720106d83fadda653a678484e55d3952f467"
Aug 01 17:24:21 minikube kubelet[1603]: E0801 17:24:21.206837    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:21 minikube kubelet[1603]: E0801 17:24:21.207139    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:24:23.207103822 +0000 UTC m=+104174.827539136 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:24:23 minikube kubelet[1603]: E0801 17:24:23.234723    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:23 minikube kubelet[1603]: E0801 17:24:23.235448    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:24:27.235381152 +0000 UTC m=+104178.855816547 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:24:27 minikube kubelet[1603]: E0801 17:24:27.296776    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:27 minikube kubelet[1603]: E0801 17:24:27.297656    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:24:35.297112938 +0000 UTC m=+104186.917548273 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:24:35 minikube kubelet[1603]: E0801 17:24:35.300043    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:35 minikube kubelet[1603]: E0801 17:24:35.300593    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:24:51.300534303 +0000 UTC m=+104202.919831289 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:24:51 minikube kubelet[1603]: E0801 17:24:51.304073    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:24:51 minikube kubelet[1603]: E0801 17:24:51.304626    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:25:23.304583931 +0000 UTC m=+104234.923880883 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:25:23 minikube kubelet[1603]: E0801 17:25:23.325296    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:25:23 minikube kubelet[1603]: E0801 17:25:23.325735    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:26:27.325619769 +0000 UTC m=+104298.944641177 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:25:53 minikube kubelet[1603]: E0801 17:25:53.371988    1603 log.go:32] "PullImage from image service failed" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Aug 01 17:25:53 minikube kubelet[1603]: E0801 17:25:53.372150    1603 kuberuntime_image.go:42] "Failed to pull image" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Aug 01 17:25:53 minikube kubelet[1603]: E0801 17:25:53.381009    1603 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:patch,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524,Command:[],Args:[patch --webhook-name=ingress-nginx-admission --namespace=$(POD_NAMESPACE) --patch-mutating=false --secret-name=ingress-nginx-admission --patch-failure-policy=Fail],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2d7gb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-patch-k2mdg_ingress-nginx(b868a4e3-d8cc-443f-b47b-909407dffa9c): ErrImagePull: rpc error: code = Canceled desc = context canceled" logger="UnhandledError"
Aug 01 17:25:53 minikube kubelet[1603]: E0801 17:25:53.383082    1603 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-k2mdg" podUID="b868a4e3-d8cc-443f-b47b-909407dffa9c"
Aug 01 17:25:53 minikube kubelet[1603]: E0801 17:25:53.817151    1603 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-k2mdg" podUID="b868a4e3-d8cc-443f-b47b-909407dffa9c"
Aug 01 17:26:22 minikube kubelet[1603]: E0801 17:26:22.548583    1603 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478" podUID="c4b6e9cf-14ac-41be-91fe-7be0f01d24db"
Aug 01 17:26:27 minikube kubelet[1603]: E0801 17:26:27.355772    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:26:27 minikube kubelet[1603]: E0801 17:26:27.355980    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:28:29.355962561 +0000 UTC m=+104420.976251639 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:28:29 minikube kubelet[1603]: E0801 17:28:29.449489    1603 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Aug 01 17:28:29 minikube kubelet[1603]: E0801 17:28:29.450442    1603 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert podName:c4b6e9cf-14ac-41be-91fe-7be0f01d24db nodeName:}" failed. No retries permitted until 2025-08-01 17:30:31.450401539 +0000 UTC m=+104543.077261438 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c4b6e9cf-14ac-41be-91fe-7be0f01d24db-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-72478" (UID: "c4b6e9cf-14ac-41be-91fe-7be0f01d24db") : secret "ingress-nginx-admission" not found
Aug 01 17:28:38 minikube kubelet[1603]: E0801 17:28:38.618357    1603 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478" podUID="c4b6e9cf-14ac-41be-91fe-7be0f01d24db"
Aug 01 17:30:29 minikube kubelet[1603]: I0801 17:30:29.951124    1603 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-2d7gb\" (UniqueName: \"kubernetes.io/projected/b868a4e3-d8cc-443f-b47b-909407dffa9c-kube-api-access-2d7gb\") pod \"b868a4e3-d8cc-443f-b47b-909407dffa9c\" (UID: \"b868a4e3-d8cc-443f-b47b-909407dffa9c\") "
Aug 01 17:30:29 minikube kubelet[1603]: I0801 17:30:29.951248    1603 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zm99z\" (UniqueName: \"kubernetes.io/projected/91494c56-487e-4368-8ce1-c5d8ad34c2dd-kube-api-access-zm99z\") pod \"91494c56-487e-4368-8ce1-c5d8ad34c2dd\" (UID: \"91494c56-487e-4368-8ce1-c5d8ad34c2dd\") "
Aug 01 17:30:29 minikube kubelet[1603]: I0801 17:30:29.962354    1603 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b868a4e3-d8cc-443f-b47b-909407dffa9c-kube-api-access-2d7gb" (OuterVolumeSpecName: "kube-api-access-2d7gb") pod "b868a4e3-d8cc-443f-b47b-909407dffa9c" (UID: "b868a4e3-d8cc-443f-b47b-909407dffa9c"). InnerVolumeSpecName "kube-api-access-2d7gb". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Aug 01 17:30:29 minikube kubelet[1603]: I0801 17:30:29.962229    1603 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/91494c56-487e-4368-8ce1-c5d8ad34c2dd-kube-api-access-zm99z" (OuterVolumeSpecName: "kube-api-access-zm99z") pod "91494c56-487e-4368-8ce1-c5d8ad34c2dd" (UID: "91494c56-487e-4368-8ce1-c5d8ad34c2dd"). InnerVolumeSpecName "kube-api-access-zm99z". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Aug 01 17:30:30 minikube kubelet[1603]: I0801 17:30:30.053553    1603 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-2d7gb\" (UniqueName: \"kubernetes.io/projected/b868a4e3-d8cc-443f-b47b-909407dffa9c-kube-api-access-2d7gb\") on node \"minikube\" DevicePath \"\""
Aug 01 17:30:30 minikube kubelet[1603]: I0801 17:30:30.054709    1603 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-zm99z\" (UniqueName: \"kubernetes.io/projected/91494c56-487e-4368-8ce1-c5d8ad34c2dd-kube-api-access-zm99z\") on node \"minikube\" DevicePath \"\""
Aug 01 17:30:30 minikube kubelet[1603]: I0801 17:30:30.656718    1603 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1d4345fbac44501728a3f8d206eaff3d53f5420c6ee287ea1811b21d1948b553"
Aug 01 17:30:30 minikube kubelet[1603]: I0801 17:30:30.668011    1603 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e6547cdb3401fab738ee220f23e4720106d83fadda653a678484e55d3952f467"
Aug 01 17:30:31 minikube kubelet[1603]: I0801 17:30:31.840847    1603 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="abe102a6045002c11406083111d36bda1f41a63a0b49392f847ce066a55cf2bd"
Aug 01 17:30:47 minikube kubelet[1603]: E0801 17:30:47.030265    1603 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9"
Aug 01 17:30:47 minikube kubelet[1603]: E0801 17:30:47.030585    1603 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9"
Aug 01 17:30:47 minikube kubelet[1603]: E0801 17:30:47.033581    1603 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:controller,Image:registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9,Command:[],Args:[/nginx-ingress-controller --election-id=ingress-nginx-leader --controller-class=k8s.io/ingress-nginx --watch-ingress-without-class=true --configmap=$(POD_NAMESPACE)/ingress-nginx-controller --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services --udp-services-configmap=$(POD_NAMESPACE)/udp-services --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:80,ContainerPort:80,Protocol:TCP,HostIP:,},ContainerPort{Name:https,HostPort:443,ContainerPort:443,Protocol:TCP,HostIP:,},ContainerPort{Name:webhook,HostPort:0,ContainerPort:8443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:LD_PRELOAD,Value:/usr/local/lib/libmimalloc.so,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{94371840 0} {<nil>} 90Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:webhook-cert,ReadOnly:true,MountPath:/usr/local/certificates/,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-nj8lw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:&Lifecycle{PostStart:nil,PreStop:&LifecycleHandler{Exec:&ExecAction{Command:[/wait-shutdown],},HTTPGet:nil,TCPSocket:nil,Sleep:nil,},StopSignal:nil,},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:*101,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-controller-67c5cb88f-72478_ingress-nginx(c4b6e9cf-14ac-41be-91fe-7be0f01d24db): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
Aug 01 17:30:47 minikube kubelet[1603]: E0801 17:30:47.035872    1603 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478" podUID="c4b6e9cf-14ac-41be-91fe-7be0f01d24db"
Aug 01 17:30:47 minikube kubelet[1603]: E0801 17:30:47.147322    1603 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478" podUID="c4b6e9cf-14ac-41be-91fe-7be0f01d24db"
Aug 01 17:37:15 minikube kubelet[1603]: I0801 17:37:15.243621    1603 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-72478" podStartSLOduration=374.187500974 podStartE2EDuration="12m56.242051821s" podCreationTimestamp="2025-08-01 17:24:19 +0000 UTC" firstStartedPulling="2025-08-01 17:30:32.010350933 +0000 UTC m=+104543.638406752" lastFinishedPulling="2025-08-01 17:37:14.063339905 +0000 UTC m=+104945.692957599" observedRunningTime="2025-08-01 17:37:15.241406286 +0000 UTC m=+104946.871023987" watchObservedRunningTime="2025-08-01 17:37:15.242051821 +0000 UTC m=+104946.871669526"
Aug 01 17:39:50 minikube kubelet[1603]: E0801 17:39:50.765570    1603 kubelet.go:2627] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.015s"
Aug 01 17:39:50 minikube kubelet[1603]: E0801 17:39:50.765656    1603 kubelet.go:2460] "Skipping pod synchronization" err="container runtime is down"
Aug 01 17:39:50 minikube kubelet[1603]: I0801 17:39:50.776161    1603 setters.go:618] "Node became not ready" node="minikube" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2025-08-01T17:39:50Z","lastTransitionTime":"2025-08-01T17:39:50Z","reason":"KubeletNotReady","message":"container runtime is down"}
Aug 01 17:39:50 minikube kubelet[1603]: E0801 17:39:50.867831    1603 kubelet.go:2460] "Skipping pod synchronization" err="container runtime is down"


==> storage-provisioner [1bac9d2b6e2c] <==
W0801 17:38:07.710007       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:07.718962       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:09.729863       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:09.739549       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:11.748097       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:11.757639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:13.768110       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:13.779505       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:15.788043       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:15.798967       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:17.806291       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:17.816021       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:19.820985       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:19.826280       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:21.833690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:38:21.847609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:50.463797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:50.651514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:52.655028       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:52.659314       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:54.663507       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:54.668817       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:56.672164       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:56.676063       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:58.679826       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:39:58.684778       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:00.689994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:00.696011       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:02.702640       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:02.711447       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:04.718877       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:04.729055       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:06.738717       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:06.747617       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:08.755103       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:08.763348       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:10.768146       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:10.773708       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:12.783688       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:12.794690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:14.803897       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:14.814918       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:16.824204       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:16.834651       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:18.844093       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:18.855463       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:20.873382       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:20.880167       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:22.888531       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:22.899886       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:24.907497       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:24.918782       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:26.928109       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:26.937806       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:28.945551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:28.954179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:30.958124       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:30.978051       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:32.982999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0801 17:40:32.989773       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [a8b80400e39e] <==
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001d0140, 0x3b9aca00, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 105 [sync.Cond.Wait, 1086 minutes]:
sync.runtime_notifyListWait(0xc000046890, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000046880)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0005102a0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc00043ec80, 0x18e5530, 0xc000047300, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0001d0180)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0001d0180, 0x18b3d60, 0xc0004c9530, 0x1, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001d0180, 0x3b9aca00, 0x0, 0x1, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001d0180, 0x3b9aca00, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 106 [sync.Cond.Wait, 1086 minutes]:
sync.runtime_notifyListWait(0xc000046a50, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000046a40)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000510420, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc00043ec80, 0x18e5530, 0xc000047300, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0001d01c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0001d01c0, 0x18b3d60, 0xc00066c000, 0x1, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0001d01c0, 0x3b9aca00, 0x0, 0x1, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0001d01c0, 0x3b9aca00, 0xc00053cd80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 51353 [select]:
golang.org/x/net/http2.(*ClientConn).Ping(0xc000498000, 0x18e55a0, 0xc0003f27e0, 0x18e55a0, 0xc0003f27e0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2463 +0x3ef
golang.org/x/net/http2.(*ClientConn).healthCheck(0xc000498000)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:715 +0xa5
created by time.goFunc
	/usr/local/go/src/time/sleep.go:180 +0x45

